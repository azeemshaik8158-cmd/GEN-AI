{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb3343f-da39-4c59-a8b5-3b5f21e4e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azeem\\AppData\\Local\\Temp\\ipykernel_31392\\1999840686.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\azeem\\AppData\\Local\\Temp\\ipykernel_31392\\1999840686.py:38: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "C:\\Users\\azeem\\AppData\\Local\\Temp\\ipykernel_31392\\1999840686.py:75: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f47f684ed647d06953.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f47f684ed647d06953.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "pdf_path = r\"C:\\Users\\azeem\\OneDrive\\Desktop\\attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Load and split PDF\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = []\n",
    "for page in loader.lazy_load():   # using sync version instead of async\n",
    "    pages.append(page)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS vectorstore\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [page.page_content for page in pages],\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "\n",
    "def chat(query, history):\n",
    "    \"\"\"Chat function that retrieves context and generates a Gemini response.\"\"\"\n",
    "\n",
    "    # Retrieve relevant docs\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Build the RAG prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant. Use the below context from a research paper to answer the user's question.\n",
    "Be factual and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Get response from Gemini\n",
    "    response = llm.invoke(prompt)\n",
    "    answer = response.content\n",
    "\n",
    "    # Update memory (conversation history)\n",
    "    memory.chat_memory.add_user_message(query)\n",
    "    memory.chat_memory.add_ai_message(answer)\n",
    "\n",
    "    # Append to visible history for UI\n",
    "    history.append({\"role\": \"user\",\"content\": query})\n",
    "    history.append({\"role\": \"assistant\",\"content\":answer})\n",
    "    return history, history\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ðŸ“˜ RAG Chatbot â€” Ask Questions from *Attention Is All You Need*\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(label=\"Ask a question about the paper\")\n",
    "\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(chat, [msg, chatbot], [chatbot, chatbot])\n",
    "    clear.click(lambda: ([], memory.clear()), None, [chatbot])\n",
    "\n",
    "\n",
    "\n",
    "demo.launch(share = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d052e-3304-4d80-b1ef-b92de072072d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
